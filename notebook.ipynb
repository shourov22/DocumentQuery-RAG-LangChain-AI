{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db1d04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-24 22:17:46.880097: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-24 22:17:47.074158: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-24 22:17:47.279432: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Diego Pavia, CEO of InnoEnergy, mentioned that the European Commission has outlined an agenda focusing on clean energy and industrial transition as vital for innovation in critical areas. He noted that numerous climate tech solutions exist, but scaling them remains a challenge due to the current dynamics.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 56, 'prompt_tokens': 2319, 'total_tokens': 2375, 'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0}, 'completion_tokens_details': {'reasoning_tokens': 0, 'audio_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_deacdd5f6f', 'id': 'chatcmpl-CqPc2DzoQhuuNl7yduu5LhanbjifT', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019b5203-ec79-7460-a646-f0bf11ab0e3f-0' usage_metadata={'input_tokens': 2319, 'output_tokens': 56, 'total_tokens': 2375, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "content=\"The report mentions Capgemini's net-zero goals as follows:\\n\\n1. Capgemini aims to be carbon neutral for its own operations by no later than 2025 and plans to become a net-zero business across its supply chain by 2030.\\n\\n2. The company has committed to becoming a net-zero business by 2040, reducing its Scope 1, 2, and 3 emissions by 90% compared to the 2019 baseline.\\n\\n3. Capgemini has near-term reduction targets for its Scopes 1, 2, and major Scope 3 impacts.\\n\\n4. The strategy to become net-zero includes supporting sustainable mobility options which have helped achieve significant reductions in business travel emissions and commuting emissions.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 147, 'prompt_tokens': 2475, 'total_tokens': 2622, 'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0}, 'completion_tokens_details': {'reasoning_tokens': 0, 'audio_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a0e9480a2f', 'id': 'chatcmpl-CqPc3Dg39ZYzOmnjJ4OJkWVX92L6U', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019b5203-f429-7da3-b554-0cb94e5a0252-0' usage_metadata={'input_tokens': 2475, 'output_tokens': 147, 'total_tokens': 2622, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS \n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "combined_text = ''\n",
    "\n",
    "files_directory = 'files'\n",
    "\n",
    "# Loop through all the files in the directory\n",
    "\n",
    "for filename in os.listdir(files_directory):\n",
    "    if filename.endswith('.pdf'):\n",
    "        # Open the pdf file\n",
    "        with pdfplumber.open(os.path.join(files_directory, filename)) as pdf:\n",
    "            # Loop through all pages in the pdf file\n",
    "            for page in pdf.pages:\n",
    "                # Extract teh text from the page and add it to the rest of the text\n",
    "                text = page.extract_text()\n",
    "                if text:\n",
    "                    combined_text += text + ' '\n",
    "\n",
    "# print(combined_text)\n",
    "\n",
    "# print(len(combined_text))\n",
    "# Length 330573  \n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=200)\n",
    "text_chunks = text_splitter.split_text(combined_text)\n",
    "\n",
    "# print(len(text_chunks))\n",
    "\n",
    "# Create an embeddings object using the HuggingFace model sentence-transformers/paraphrase-MiniLM-L6-v2\n",
    "\n",
    "#embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-MiniLM-L6-v2\")\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "# Ingest the documents into Vector store (FAISS)\n",
    "\n",
    "db = FAISS.from_texts(text_chunks, embeddings)\n",
    "\n",
    "# Verify the index of the FAISS vector store to confirm it is populated\n",
    "# print(db.index.ntotal)    \n",
    "\n",
    "# Convert the FAISS vector store into a retriever that can return documents for a given query\n",
    "# Instantiate the retriever by calling the as_retriever method on the vector store\n",
    "\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "# The retriever finds the 12 most similar chunks\n",
    "retriever.search_kwargs[\"k\"] = 12\n",
    "\n",
    "# Check the default top_k value set at langchain which defines how many chunks it will return (in case it is set)\n",
    "# print(retriever.search_kwargs)\n",
    "\n",
    "# Test the retriever\n",
    "# print(retriever.invoke(\"Who is the CEO of InnoEnergy?\"))\n",
    "\n",
    "# Create a RAG prompt template to include both the question and the context to answer the question posted by the user.\n",
    "# The goal of defining the prompt is to translate user input and parameters into clear instrcutions for the Open AI model. \n",
    "# It will help the model to understand the context and answer the question posted by the user.\n",
    "\n",
    "template = \"\"\"\n",
    "    Answer the question based only on the following context:\n",
    "    {context}\n",
    "\n",
    "    Question: {input}\n",
    "\"\"\"\n",
    "\n",
    "# Create the prompt based on the template. This converts the raw string into a LangChain prompt object that can be fed into an LLM chain.\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Setup the LLM (gpt-3.5-turbo) and RAG retrieval chain; an api key (OPENAI_API_KEY; available through .env) for the AI model would be required\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\") #gpt-3.5-turbo\n",
    "\n",
    "# Construct the chain that will be used to generate a response based on a set of documents and a question\n",
    "# In LangChain, a \"stuff\" chain takes all retrieved documents and \"Stuff\" them into the prompt. Then sends them to the LLM to get an answer\n",
    "# It does not retrieve documents. It only formats, combines and sends to LLM.\n",
    "\n",
    "# combine_docs_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "# Create the retrieval chain(RAG pipeline).\n",
    "# It connects two things: Retriever fetches relevant documents from the vector store and combine_docs_chain uses those documents plus prompt to answer the question\n",
    "# The final chain does:\n",
    "#    - Take user question\n",
    "#    - Use the retriever to get relevant chunks\n",
    "#    - Pass those chunks into the \"stuff\" chain\n",
    "#    - Generate the final answer\n",
    "\n",
    "# rag_chain = create_retrieval_chain(retriever, combine_docs_chain)\n",
    "\n",
    "# LangChain Expression Language (LCEL) RAG pipeline\n",
    "\n",
    "rag_chain = ( \n",
    "    { \n",
    "        \"context\": retriever, # retrieves relevant chunks \n",
    "        \"input\": RunnablePassthrough() # passes user question through \n",
    "        } \n",
    "        | prompt # formats prompt \n",
    "        | llm # generates answer \n",
    ")\n",
    "\n",
    "# Invoke the chain by submitting queries.\n",
    "\n",
    "print(rag_chain.invoke(\"What did the Diego Pavia (D.P.), CEO of InnoEnergy mention about the expected dynamics in climate tech in the next few months?\")) \n",
    "print(rag_chain.invoke(\"What is mentioned on Capgemini's net-zero goals in the report?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9660af20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
